{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90266419",
   "metadata": {},
   "source": [
    "# To Reason or Not?\n",
    "\n",
    "The TRON notebook is a pipeline to generate synthetic data for the To Reason or Not (TRON) Dataset. Reasoning models are token-heavy and compute-heavy. What if we could create an agentic pipeline that leveraged a Reasoning Agent who would quickly determine whether or not the user's query was complex enough to justify a more thorough reasoning task?\n",
    "\n",
    "![](./images/tron-arch.jpg)\n",
    "\n",
    "Imagine a case where the underlying model of the TRON-trained agent were to be restricted to a finite token limit to determine whether or not a more expensive reasoning transaction was necessary to sufficiently respond to the input prompt and the downstream response generation agents were given a more liberal allocation, because the user could be sure that only meritous tasks were reaching it. The TRON-trained agent could be taught to support an arbitrary number of downstream response agents as well, making the implications of such an agent very beneficial to research fields. \n",
    "\n",
    "Further, this architecture shifts the burden of the resource allocation to the agentic model system itself rather than the user. An inexperienced, programmatic, or high-frequency user may not have the time or knowledge to individually assess the proper model type to use, thus exposing themselves to unnessesary token consumption. The TRON architecture has potential to help startups use their limited resources more efficiently.\n",
    "\n",
    "The TRON Dataset is a step towards the reality visualized and discribed above and represents a tangible step forward to productive, safe, and user-oriented AI systems. TRON can be used to fine tine a reasoning model to quickly reason through whether or not it makes sense to invest in a full reasoning task or a simple LLM completion task makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7418f",
   "metadata": {},
   "source": [
    "## Data Creation Pipeline\n",
    "\n",
    "The TRON notebook uses a two-step generation process to minimize the dataset creation cost. Please consider the visualization below for a better understanding of how this process works at a high level.\n",
    "\n",
    "![Pipeline architecture](./images/pipeline-architecture.jpg)\n",
    "\n",
    "Each of the steps illustrated above has a detailed explaination about what is happening, as it happens. Most steps have the ability for user customization for use in other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98881b4",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "Quickly navigate throughout the training notebook using this section.\n",
    "\n",
    "1. [Introduction](#to-reason-or-not)\n",
    "2. [Data Creation Pipeline](#data-creation-pipeline)\n",
    "3. [Configuration](#configuration)\n",
    "4. [Prompt Engineering](#prompt-engineering)\n",
    "5. [Helper Functions](#helper-functions)\n",
    "6. [Sampling](#sampling)\n",
    "7. [Question Generation Loop](#question-generation-loop)\n",
    "8. [Output Management](#output-management)\n",
    "9. [Visualize Dataset](#visualize-dataset)\n",
    "10. [Setup for Reasoning](#setup-for-reasoning)\n",
    "11. [Reasoning Generation Loop](#reasoning-generation-loop)\n",
    "12. [Update Output](#update-output)\n",
    "13. [Reasoning Dataset Analytics](#reasoning-dataset-analytics)\n",
    "14. [Upload to Hugging Face Hub](#upload-to-hugging-face-hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "%pip install datasets huggingface_hub matplotlib pandas seaborn scipy transformers together torch tqdm -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8070d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from together import Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531f00d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The TRON notebook supports a highly customizable generation configuration to prepare your model to specialize in a specific field or have a broad base of general knowledge.\n",
    "\n",
    "In order to reduce bias and increase applicability across a broad set of domains, you can customize this pipeline according to the parameters below. All inputs are randomized within the boundaries that you define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de633f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # iteration count\n",
    "    # configure the amount of rows of data to generate\n",
    "    \"num_iterations\": 100,\n",
    "\n",
    "    # model proportion bounds\n",
    "    # reduce dataset bias by ensuring diverse generation\n",
    "    # during generation, model is selected randomly\n",
    "    # define the min / max proportions that any individual model may be polled during generation\n",
    "    \"min_poll\": 0.15,\n",
    "    \"max_poll\": 0.4,\n",
    "\n",
    "    # difficulty proportion bounds\n",
    "    # during generation, question difficulty is selected randomly\n",
    "    # define the min / max proportions of each difficulty to generate more or less complex datasets\n",
    "    \"min_easy\": 0.2,\n",
    "    \"max_easy\": 0.4,\n",
    "    \"min_normal\": 0.2,\n",
    "    \"max_normal\": 0.4,\n",
    "    \"min_hard\": 0.2,\n",
    "    \"max_hard\": 0.4,\n",
    "\n",
    "    # category usage bounds\n",
    "    # during generation, category is selected randomly\n",
    "    # define the mix / max proportions of each category to generate more or less topical datasets\n",
    "    \"min_cat\": 0.075,\n",
    "    \"max_cat\": 0.15,\n",
    "\n",
    "    # define custom categories\n",
    "    # during generation, categories are selected at random from this defined list\n",
    "    # increase or decrease the number of categories to increase dataset diversity\n",
    "    # choose several variants of the same category to target a specific domain\n",
    "    \"allowed_categories\": [\"geography\", \"logic\", \"ancient_history\", \"calculus\", \"politics\", \"pop_culture\", \"language\", \"philosophy\", \"military_sciences\", \"ethics\"],\n",
    "\n",
    "    # output utils\n",
    "    # notebook supports export to csv for local use\n",
    "    \"save_to_csv\": True,\n",
    "    \"csv_filename\": \"tron_demo.csv\",\n",
    "\n",
    "    # reasoning configuration\n",
    "    # modify the reasoning loop for your use case\n",
    "    \"reasoning_model\": \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"max_tokens\": 500,\n",
    "    \"max_queries_per_minute\": 3,\n",
    "    \"rate_limit_window_seconds\": 60\n",
    "}\n",
    "\n",
    "# available models: https://www.together.ai/inference\n",
    "models = [\n",
    "    \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"deepseek-ai/DeepSeek-V3\",\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"microsoft/WizardLM-2-8x22B\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe58ee8",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "The default prompt supports the injection of `difficulty` and `category` parameters to focus generation on the target domains. The default query here has proven to work well for the task of generating predictable and structured output, but you may wish to update it according to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3710f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to control \n",
    "def get_sample_within_bounds(counts, options, min_prop, max_prop, total):\n",
    "    eligible = []\n",
    "    min_count = int(total * min_prop)\n",
    "    max_count = int(total * max_prop)\n",
    "\n",
    "    for option in options:\n",
    "        count = counts[option]\n",
    "        if count < max_count:\n",
    "            eligible.append(option)\n",
    "\n",
    "    return random.choice(eligible) if eligible else None\n",
    "\n",
    "# function contain question generation prompt\n",
    "# modify this prompt to address your specific use case if desired\n",
    "def inject_prompt(difficulty, category):\n",
    "    return f\"\"\"\n",
    "You are generating synthetic data for evaluating language models.\n",
    "\n",
    "Your task is to generate ONE creative and diverse question that could be used to evaluate a language model.\n",
    "Avoid repetition and be imaginative ‚Äî use a wide variety of topics and question formats.\n",
    "\n",
    "The question should be about **{category}** and of **{difficulty}** difficulty.\n",
    "\n",
    "In addition to the question, also indicate:\n",
    "1. Whether it requires reasoning (e.g., logical steps, inference).\n",
    "2. A one-word category (e.g., geography, logic, history, math, language, etc.).\n",
    "3. A one-word difficulty level (easy, normal, or hard).\n",
    "\n",
    "Respond ONLY with a valid JSON object, and do NOT include any explanation or markdown formatting.\n",
    "Do NOT wrap your response in a ```json code block``` ‚Äî return only raw JSON.\n",
    "\n",
    "Format:\n",
    "{{\n",
    "  \"question\": \"What is the capital of France?\",\n",
    "  \"use_reasoning\": false,\n",
    "  \"category\": \"geography\",\n",
    "  \"difficulty\": \"easy\"\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15586f91",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions are used by the main algorithm to support parameters defined in the [Configuration](#configuration) and extract structured output from the models. In order to use this pipeline out of the box, it is recommended not to modify these functions unless you wish to modify overall notebook behavior (which is welcomed and encouraged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to control amount of time each model is polled\n",
    "def get_sample_within_bounds(counts, options, min_prop, max_prop, total):\n",
    "    eligible = []\n",
    "    min_count = int(total * min_prop)\n",
    "    max_count = int(total * max_prop)\n",
    "\n",
    "    for option in options:\n",
    "        count = counts[option]\n",
    "        if count < max_count:\n",
    "            eligible.append(option)\n",
    "\n",
    "    return random.choice(eligible) if eligible else None\n",
    "\n",
    "# function to extract structure json content from llm responses\n",
    "def extract_json_block(text):\n",
    "    \"\"\"\n",
    "    Extracts the first valid JSON object from a string using regex.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r\"^```json|```$\", \"\", text.strip(), flags=re.IGNORECASE).strip()\n",
    "    match = re.search(r\"\\{[\\s\\S]*?\\}\", cleaned)\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(match.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426bb29",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "In [Configuration](#configuration), you defined certain parameters associated with which models would be polls at which frequency, the categories that generated questions would consider, and the distribution of question difficulty. \n",
    "\n",
    "This section of the notebook configures the pipeline to respect those parameters during the [Question Generation Loop](#question-generation-loop). For most use cases, these functions do not need to be modified, unless you would like to change this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62126317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling functions which respect the user-defined parameters in the config\n",
    "def get_model_sample(model_counts, total_iterations):\n",
    "    return get_sample_within_bounds(\n",
    "        counts=model_counts,\n",
    "        options=models,\n",
    "        min_prop=config[\"min_poll\"],\n",
    "        max_prop=config[\"max_poll\"],\n",
    "        total=total_iterations\n",
    "    )\n",
    "\n",
    "def get_difficulty_sample(difficulty_counts, total_iterations):\n",
    "    options = [\"easy\", \"normal\", \"hard\"]\n",
    "    bounds = {\n",
    "        \"easy\": (config[\"min_easy\"], config[\"max_easy\"]),\n",
    "        \"normal\": (config[\"min_normal\"], config[\"max_normal\"]),\n",
    "        \"hard\": (config[\"min_hard\"], config[\"max_hard\"]),\n",
    "    }\n",
    "\n",
    "    eligible = [\n",
    "        d for d in options\n",
    "        if difficulty_counts[d] < int(total_iterations * bounds[d][1])\n",
    "    ]\n",
    "    return random.choice(eligible) if eligible else None\n",
    "\n",
    "def get_category_sample(category_counts, total_iterations):\n",
    "    return get_sample_within_bounds(\n",
    "        counts=category_counts,\n",
    "        options=config[\"allowed_categories\"],\n",
    "        min_prop=config[\"min_cat\"],\n",
    "        max_prop=config[\"max_cat\"],\n",
    "        total=total_iterations\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6876cd9",
   "metadata": {},
   "source": [
    "# Question Generation Loop\n",
    "\n",
    "The first generation loop generates the base questions based on the [Configuration](#configuration). We ask the defined models to create and categorize a question, assign it a difficulty level, and specify whether answering the question requires reasoning or not.\n",
    "\n",
    "The goal is the synthesize data that can be used to prompt a reasoning model to then explain why a given question does or does not require further reasoning. This is the core component of the TRON dataset and is used to finetune a downstream reasoning model to recognize whether the user's prompt requires reasoning or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ba316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "api_key = os.getenv(\"TOGETHER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630304fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate together ai client\n",
    "client = Together(api_key=api_key)\n",
    "results = []\n",
    "\n",
    "# create dictionaries to respect configration parameters\n",
    "model_counts = defaultdict(int)\n",
    "difficulty_counts = defaultdict(int)\n",
    "category_counts = defaultdict(int)\n",
    "\n",
    "print(f\"\\nüîÅ Generating {config['num_iterations']} samples...\")\n",
    "\n",
    "# question generation loop\n",
    "for i in tqdm(range(config[\"num_iterations\"])):\n",
    "    model = get_model_sample(model_counts, config[\"num_iterations\"])\n",
    "    difficulty = get_difficulty_sample(difficulty_counts, config[\"num_iterations\"])\n",
    "    category = get_category_sample(category_counts, config[\"num_iterations\"])\n",
    "\n",
    "    if not all([model, difficulty, category]):\n",
    "        print(f\"‚ö†Ô∏è Skipping iteration {i} ‚Äî no valid model or category/difficulty available.\")\n",
    "        continue\n",
    "\n",
    "    prompt = inject_prompt(difficulty, category)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "        data = extract_json_block(reply)\n",
    "\n",
    "        if not data or not all(k in data for k in [\"question\", \"use_reasoning\", \"category\", \"difficulty\"]):\n",
    "            raise ValueError(\"Response missing required fields.\")\n",
    "\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"question\": data[\"question\"],\n",
    "            \"use_reasoning\": data[\"use_reasoning\"],\n",
    "            \"category\": data[\"category\"],\n",
    "            \"difficulty\": data[\"difficulty\"]\n",
    "        })\n",
    "\n",
    "        model_counts[model] += 1\n",
    "        difficulty_counts[difficulty] += 1\n",
    "        category_counts[category] += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error with model '{model}': {e}\")\n",
    "        print(f\"‚ö†Ô∏è Raw reply: {reply if 'reply' in locals() else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3602648",
   "metadata": {},
   "source": [
    "## Output Management\n",
    "\n",
    "In the [Configuration](#configuration), you can set `save_to_csv` to `True` if you'd like to save a local copy of the generated dataset. Otherwise, the generated dataset is stored in-mem as a DataFrame for use during [Reasoning Generation Loop](#reasoning-generation-loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d86d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save generation results as a dataframe for further use\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# check save_to_csv parameter in config to control this behavior\n",
    "if config[\"save_to_csv\"] and not df.empty:\n",
    "    df.to_csv(config[\"csv_filename\"], index=False)\n",
    "    print(f\"\\nüìÅ Results saved to: {config['csv_filename']}\")\n",
    "elif config[\"save_to_csv\"]:\n",
    "    print(\"\\n‚ö†Ô∏è No results to save ‚Äî CSV not written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f8ed0b",
   "metadata": {},
   "source": [
    "## Visualize Dataset\n",
    "\n",
    "You can use the visualizations commands below out of the box to get a simple report of the output of your generation pipeline.\n",
    "\n",
    "The following information is visualized:\n",
    "1. Count of Category, stacked segments of difficulty.\n",
    "2. Proportion of `use_reasoning` values.\n",
    "3. Count of number of times each model was polled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color theme\n",
    "plt.style.use(\"seaborn-v0_8-pastel\")\n",
    "\n",
    "# dashboarding format\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# count of category stacked by difficulty\n",
    "pivot = df.pivot_table(index=\"category\", columns=\"difficulty\", aggfunc=\"size\", fill_value=0)\n",
    "pivot.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    ax=axes[0],\n",
    "    color=sns.color_palette(\"pastel\", n_colors=len(pivot.columns))\n",
    ")\n",
    "axes[0].set_title(\"Questions by Category √ó Difficulty\")\n",
    "axes[0].set_xlabel(\"Category\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].tick_params(axis='x', labelrotation=30, labelsize=6)\n",
    "axes[0].yaxis.set_major_locator(ticker.MaxNLocator(integer=True))  # ‚úÖ Only integer ticks\n",
    "\n",
    "# proportion of use_reasoning values\n",
    "reasoning_colors = {\n",
    "    True: \"#86c5d8\",   # pastel blue\n",
    "    False: \"#a5d6a7\"   # pastel green\n",
    "}\n",
    "reasoning_counts = df[\"use_reasoning\"].value_counts()\n",
    "axes[1].pie(\n",
    "    reasoning_counts,\n",
    "    labels=[\"True\", \"False\"],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=[reasoning_colors.get(k) for k in reasoning_counts.index],\n",
    "    startangle=90\n",
    ")\n",
    "axes[1].set_title(\"Reasoning Recommended\")\n",
    "axes[1].axis('equal')\n",
    "\n",
    "# model usage bar chart\n",
    "model_counts = df[\"model\"].value_counts()\n",
    "truncated_labels = [m[:25] + \"‚Ä¶\" if len(m) > 25 else m for m in model_counts.index]\n",
    "bar_colors = sns.color_palette(\"pastel\", n_colors=len(model_counts))\n",
    "\n",
    "axes[2].bar(truncated_labels, model_counts.values, color=bar_colors)\n",
    "axes[2].set_title(\"Samples by Model\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].tick_params(axis='x', labelrotation=30, labelsize=8)\n",
    "axes[2].yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "# display charts\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde15b4",
   "metadata": {},
   "source": [
    "## Setup for Reasoning\n",
    "\n",
    "This section of the notebook setups up the [Reasoning Generation Loop](#reasoning-generation-loop). These variables are visualized in [Reasoning Dataset Analytics](#reasoning-dataset-analytics) to give you an understanding of how well your configuation is working. More information about interpreting those graphs can be found in that section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2477ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tokenizer for token counting\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# initialize memory dictionaries\n",
    "reasoning_strings = []\n",
    "token_lengths = []\n",
    "token_limit_hits = []\n",
    "response_times = []\n",
    "request_timestamps = deque()\n",
    "\n",
    "# time and error counting\n",
    "error_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# extract configuration variables\n",
    "max_tokens = config[\"max_tokens\"]\n",
    "max_qpm = config[\"max_queries_per_minute\"]\n",
    "window_seconds = config[\"rate_limit_window_seconds\"]\n",
    "reasoning_model = config[\"reasoning_model\"]\n",
    "\n",
    "print(f\"\\nü§î Reasoning Generation is configured to use: {reasoning_model}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346f82a",
   "metadata": {},
   "source": [
    "## Reasoning Generation Loop\n",
    "\n",
    "This loop iterates through each line of the output dataframe from [Question Generation Loop](#question-generation-loop) and polls [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) to generate a `<think>` block (representing the reasoning-only segment of an answer) with fewer than the defined amount of tokens. The `max_tokens` parameter is configurable and injected into the query sent to the model itself to ensure that the the limit is communicated literally as well as programatically, this has been observed to generate fewer incomplete or cut-off responses.\n",
    "\n",
    "> Please note, regardless of prompting, `max_tokens` configurations of less than 256 tokens appear to overflow and be cut off nearly 100% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Reasoning via {reasoning_model}\"):\n",
    "    question = row[\"question\"]\n",
    "    label = row[\"use_reasoning\"]\n",
    "    category = row[\"category\"]\n",
    "    label_str = \"does\" if label else \"does not\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "<think>\n",
    "You are a minimal reasoning agent tasked with quickly evaluating why a question requires multi-step logical reasoning or not. Do not attempt to answer the question, but simply explain why the question does or does not require reasoning.\n",
    "\n",
    "The following question {label_str} require reasoning.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please do not think for more than {max_tokens} tokens.\n",
    "\n",
    "Remember, your task is not to answer the question, but simply explain why the question requires reasoning or not.\n",
    "</think>\n",
    "\"\"\".strip()\n",
    "\n",
    "    # rate limiting controls\n",
    "    now = time.time()\n",
    "    request_timestamps.append(now)\n",
    "    while len(request_timestamps) > max_qpm:\n",
    "        if now - request_timestamps[0] < window_seconds:\n",
    "            sleep_time = window_seconds - (now - request_timestamps[0])\n",
    "            print(f\"‚è≥ Rate limit hit ‚Äî sleeping {sleep_time:.1f}s...\")\n",
    "            time.sleep(sleep_time)\n",
    "        else:\n",
    "            request_timestamps.popleft()\n",
    "\n",
    "    try:\n",
    "        req_start = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=reasoning_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stop=[\"</think>\"],\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        req_end = time.time()\n",
    "        response_times.append(req_end - req_start)\n",
    "\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        full_reasoning = response_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip() + \" </think>\"\n",
    "        reasoning_strings.append(full_reasoning)\n",
    "\n",
    "        token_count = len(tokenizer.encode(response_text))\n",
    "        token_lengths.append(token_count)\n",
    "        token_limit_hits.append(token_count >= max_tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(f\"‚ö†Ô∏è Error on question: {question[:60]}... ‚Üí {e}\")\n",
    "        print(\"‚è±Ô∏è Retrying after 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "        try:\n",
    "            retry_start = time.time()\n",
    "            response = client.chat.completions.create(\n",
    "                model=reasoning_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                stop=[\"</think>\"],\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            retry_end = time.time()\n",
    "            response_times.append(retry_end - retry_start)\n",
    "\n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            full_reasoning = response_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip() + \" </think>\"\n",
    "            reasoning_strings.append(full_reasoning)\n",
    "\n",
    "            token_count = len(tokenizer.encode(response_text))\n",
    "            token_lengths.append(token_count)\n",
    "            token_limit_hits.append(token_count >= max_tokens)\n",
    "\n",
    "        except Exception as e2:\n",
    "            error_count += 1\n",
    "            print(f\"‚ùå Retry failed ‚Üí {e2}\")\n",
    "            reasoning_strings.append(\"<think>Error generating reasoning</think>\")\n",
    "            token_lengths.append(0)\n",
    "            token_limit_hits.append(False)\n",
    "            response_times.append(0.0)\n",
    "\n",
    "# ‚úÖ Finalize\n",
    "df[\"reasoning\"] = reasoning_strings\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa27fc2",
   "metadata": {},
   "source": [
    "## Update Output\n",
    "\n",
    "The `reasoning` column is added to the in-memory DataFrame during the [Reasoning Generation Loop](#reasoning-generation-loop). If you have configured `'save_to_csv':True` in the [Configuration](#configuration), the following cell appends a formated, single-line `reasoning` column to your local CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b815994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check save_to_csv parameter from configuration and update local csv accordingly\n",
    "# please note, the process will error if you've deleted the underlying csv between its creation and this step\n",
    "if config.get(\"save_to_csv\", False):\n",
    "    csv_path = config[\"csv_filename\"]\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        try:\n",
    "            df.to_csv(csv_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "            print(f\"\\n‚úÖ Reasoning column appended and saved to existing CSV: {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving to CSV: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è CSV file not found: {csv_path}. No file saved, but reasoning is attached to DataFrame.\")\n",
    "else:\n",
    "    print(\"üìÅ save_to_csv is False ‚Äî skipping file write, reasoning is only in-memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db5593",
   "metadata": {},
   "source": [
    "# Reasoning Dataset Analytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color theme\n",
    "plt.style.use(\"seaborn-v0_8-pastel\")\n",
    "\n",
    "# build token_lengths distribution histogram\n",
    "bins = np.linspace(min(token_lengths), max(token_lengths), 20)\n",
    "counts, edges = np.histogram(token_lengths, bins=bins)\n",
    "centers = (edges[:-1] + edges[1:]) / 2\n",
    "\n",
    "# ignore 0 bins\n",
    "centers_nonzero = centers[counts > 0]\n",
    "counts_nonzero = counts[counts > 0]\n",
    "\n",
    "# count bins\n",
    "total_time = end_time - start_time\n",
    "time_per_response = total_time / len(df)\n",
    "mean_tokens = np.mean(token_lengths)\n",
    "median_tokens = np.median(token_lengths)\n",
    "\n",
    "# generation kpis\n",
    "max_tokens_used = np.max(token_lengths)\n",
    "min_tokens_used = np.min(token_lengths)\n",
    "response_times = [time_per_response for _ in token_lengths]\n",
    "max_time = np.max(response_times)\n",
    "min_time = np.min(response_times)\n",
    "\n",
    "# set dashboard layout\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 5))\n",
    "\n",
    "# token length distribution (line chart)\n",
    "if len(centers_nonzero) >= 4:\n",
    "    x_smooth = np.linspace(centers_nonzero.min(), centers_nonzero.max(), 300)\n",
    "    spline = make_interp_spline(centers_nonzero, counts_nonzero, k=3)\n",
    "    y_smooth = spline(x_smooth)\n",
    "\n",
    "    axes[0].plot(x_smooth, y_smooth, color=\"#ffb481\", linewidth=3.5, label=\"Smoothed Distribution\")\n",
    "    axes[0].scatter(centers_nonzero, counts_nonzero, color=\"#ffb481\", edgecolor=\"black\", zorder=3, label=\"Data Points\")\n",
    "else:\n",
    "    axes[0].plot(centers_nonzero, counts_nonzero, color=\"#ffb481\", linewidth=3.5, label=\"Token Lengths\")\n",
    "    axes[0].scatter(centers_nonzero, counts_nonzero, color=\"#ffb481\", edgecolor=\"black\", zorder=3)\n",
    "\n",
    "axes[0].axvline(mean_tokens, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=f\"Mean ({mean_tokens:.1f})\")\n",
    "axes[0].axvline(median_tokens, color=\"black\", linestyle=\":\", linewidth=1.5, label=f\"Median ({median_tokens:.1f})\")\n",
    "\n",
    "axes[0].set_title(\"Token Length Distribution\")\n",
    "axes[0].set_xlabel(\"Tokens\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# token limit (pie chart))\n",
    "limit_hit_counts = {\n",
    "    \"Hit Limit\": sum(token_limit_hits),\n",
    "    \"Within Limit\": len(token_limit_hits) - sum(token_limit_hits)\n",
    "}\n",
    "axes[1].pie(\n",
    "    limit_hit_counts.values(),\n",
    "    labels=limit_hit_counts.keys(),\n",
    "    autopct='%1.1f%%',\n",
    "    colors=[\"#86c5d8\", \"#a5d6a7\"],\n",
    "    startangle=90\n",
    ")\n",
    "axes[1].set_title(\"Token Limit Hit\")\n",
    "\n",
    "# kpis (kpis)\n",
    "axes[2].axis(\"off\")\n",
    "kpi_x = 0.05\n",
    "\n",
    "# number of errors\n",
    "axes[2].text(kpi_x, 0.95, \"‚Üì Errors\", fontsize=12, fontweight=\"bold\", color=\"#d73027\")\n",
    "axes[2].text(kpi_x, 0.90, f\"{error_count}\", fontsize=22, fontweight=\"bold\")\n",
    "\n",
    "# total execution time\n",
    "axes[2].text(kpi_x, 0.80, \"‚è± Total Time (s)\", fontsize=12, fontweight=\"bold\", color=\"#1a9850\")\n",
    "axes[2].text(kpi_x, 0.75, f\"{total_time:.1f}\", fontsize=20, fontweight=\"bold\")\n",
    "\n",
    "# avg response time\n",
    "axes[2].text(kpi_x, 0.67, \"‚è± Avg per Response (s)\", fontsize=12, fontweight=\"bold\", color=\"#4575b4\")\n",
    "axes[2].text(kpi_x, 0.62, f\"{time_per_response:.2f}\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# longest response time\n",
    "axes[2].text(kpi_x, 0.53, \"‚è± Longest Response (s)\", fontsize=11, fontweight=\"bold\", color=\"#ab47bc\")\n",
    "axes[2].text(kpi_x, 0.49, f\"{max_time:.2f}\", fontsize=14)\n",
    "\n",
    "# shortest response time\n",
    "axes[2].text(kpi_x, 0.42, \"‚è± Shortest Response (s)\", fontsize=11, fontweight=\"bold\", color=\"#8e24aa\")\n",
    "axes[2].text(kpi_x, 0.38, f\"{min_time:.2f}\", fontsize=14)\n",
    "\n",
    "# longest response tokens\n",
    "axes[2].text(kpi_x, 0.30, \"‚Üë Longest Response (tokens)\", fontsize=11, fontweight=\"bold\", color=\"#0288d1\")\n",
    "axes[2].text(kpi_x, 0.26, f\"{max_tokens_used}\", fontsize=14)\n",
    "\n",
    "# shortest response tokens\n",
    "axes[2].text(kpi_x, 0.18, \"‚Üì Shortest Response (tokens)\", fontsize=11, fontweight=\"bold\", color=\"#039be5\")\n",
    "axes[2].text(kpi_x, 0.14, f\"{min_tokens_used}\", fontsize=14)\n",
    "\n",
    "# display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094a518",
   "metadata": {},
   "source": [
    "# Upload to Hugging Face Hub\n",
    "\n",
    "Now that you've generated your own dataset using the TRON pipeline, upload it the Hugging Face Hub so that others can use it to create powerful meta-reasoning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create remote repo\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=\"ZennyKenny/TRON-dataset-demo\", repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "# upload to remote\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "hf_dataset.push_to_hub(\"ZennyKenny/TRON-dataset-demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
