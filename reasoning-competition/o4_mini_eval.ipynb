{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cef14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hugging Face token loaded.\n",
      "‚úÖ OpenAI key loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "dotenv_path = os.path.join(os.path.dirname(os.getcwd()), \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "assert hf_token, \"‚ùå HF_API_KEY not found in environment.\"\n",
    "print(\"‚úÖ Hugging Face token loaded.\")\n",
    "\n",
    "openai_token = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert openai_token, \"‚ùå OPENAI_API_KEY not found\"\n",
    "print(\"‚úÖ OpenAI key loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df643aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"ZennyKenny/cosa-benchmark-dataset\",\n",
    "    split=\"train\",\n",
    "    token=hf_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0784ab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded Xet-backed dataset:\n",
      "   index                                               code    language  \\\n",
      "0      1  public class UserController {\\n    @Autowired\\...        Java   \n",
      "1      2  def authenticate_user(username, password):\\n  ...      Python   \n",
      "2      3  const express = require('express');\\nconst mys...  JavaScript   \n",
      "3      4  import java.sql.*;\\n\\npublic class DatabaseMan...        Java   \n",
      "4      5  const express = require('express');\\nconst mys...  JavaScript   \n",
      "\n",
      "  difficulty vulnerability_type  \\\n",
      "0       easy      SQL Injection   \n",
      "1       easy      SQL Injection   \n",
      "2       easy      SQL Injection   \n",
      "3       easy      SQL Injection   \n",
      "4       easy      SQL Injection   \n",
      "\n",
      "                                   weakness_solution  \\\n",
      "0  <think> Okay, let's take a look at this Java c...   \n",
      "1  <think> Okay, let's take a look at this Python...   \n",
      "2  <think> Okay, let's take a look at this JavaSc...   \n",
      "3  <think> Okay, let's take a look at this Java c...   \n",
      "4  <think> Okay, let's take a look at this JavaSc...   \n",
      "\n",
      "                                   weakness_analysis  \\\n",
      "0  <think><think> Okay, let's take a look at this...   \n",
      "1  <think><think> Okay, let's take a look at this...   \n",
      "2  <think><think> Okay, let's take a look at this...   \n",
      "3  <think><think> Okay, let's take a look at this...   \n",
      "4  <think><think> Okay, let's take a look at this...   \n",
      "\n",
      "                                  solution_statement  \\\n",
      "0  Use parameterized queries with prepared statem...   \n",
      "1  Use parameterized queries to safely handle use...   \n",
      "2  Use parameterized queries to prevent SQL injec...   \n",
      "3  Use prepared statements to safely handle user ...   \n",
      "4  Use parameterized queries to safely handle use...   \n",
      "\n",
      "                                           safe_code  \n",
      "0  ```java\\npublic class UserController {\\n    @A...  \n",
      "1  ```python\\nimport sqlite3\\n\\ndef authenticate_...  \n",
      "2  ```javascript\\nconst express = require('expres...  \n",
      "3  ```java\\nimport java.sql.*;\\n\\npublic class Da...  \n",
      "4  ```javascript\\nconst express = require('expres...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = dataset.to_pandas()\n",
    "print(\"‚úÖ Loaded Xet-backed dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa78602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(code):\n",
    "    return f\"\"\"\n",
    "You are a security-focused AI assistant. You will be given a code snippet. Your task is:\n",
    "\n",
    "1. Identify the vulnerability in the code.\n",
    "2. Explain the issue in **one short sentence**.\n",
    "3. Provide a **corrected version of the code** that resolves the vulnerability.\n",
    "\n",
    "Return your answer using the following format exactly:\n",
    "\n",
    "Explanation: <your one-sentence explanation>\n",
    "Fixed Code:\n",
    "<your corrected code here>\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a082f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running o4 mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [14:49<00:00,  4.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key=openai_token)\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Running o4 mini\"):\n",
    "    code = row[\"code\"]\n",
    "    idx = row[\"index\"]\n",
    "\n",
    "    try:\n",
    "        prompt = build_prompt(code)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=512\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        explanation = \"\"\n",
    "        fixed_code = \"\"\n",
    "\n",
    "        for line in content.splitlines():\n",
    "            if line.startswith(\"Explanation:\"):\n",
    "                explanation = line.replace(\"Explanation:\", \"\").strip()\n",
    "            elif line.startswith(\"Fixed Code:\"):\n",
    "                fixed_code = content.split(\"Fixed Code:\")[1].strip()\n",
    "                break\n",
    "\n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"model_explanation\": explanation,\n",
    "            \"model_fix\": fixed_code\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error on row {idx}: {e}\")\n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"model_explanation\": \"ERROR\",\n",
    "            \"model_fix\": \"\"\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd13d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 200 results to o4_mini_eval_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_json(\"o4_mini_eval_results.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(results_df)} results to o4_mini_eval_results.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8b797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f261e61ae6204370b1dd37fd199e08c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded and saved benchmark as llm-code-safety-benchmark.jsonl\n"
     ]
    }
   ],
   "source": [
    "# don't need to run this if results already converted and saved locally\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Load benchmark from HF\n",
    "dataset = load_dataset(\"ZennyKenny/cosa-benchmark-dataset\", split=\"train\", token=hf_token)\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Save locally\n",
    "df.to_json(\"llm-code-safety-benchmark.jsonl\", orient=\"records\", lines=True)\n",
    "print(\"‚úÖ Downloaded and saved benchmark as llm-code-safety-benchmark.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b0a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Unified dataset ready with 200 rows.\n"
     ]
    }
   ],
   "source": [
    "truth = pd.read_json(\"llm-code-safety-benchmark.jsonl\", lines=True)\n",
    "\n",
    "results = pd.read_json(\"o4_mini_eval_results.jsonl\", lines=True)\n",
    "\n",
    "results = results.rename(columns={\n",
    "    \"explanation\": \"model_explanation\",\n",
    "    \"fixed_code\": \"model_fix\"\n",
    "})\n",
    "\n",
    "df = pd.merge(truth, results, on=\"index\")\n",
    "print(\"‚úÖ Unified dataset ready with\", len(df), \"rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a374494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import difflib\n",
    "\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "difficulty_weights = {\"easy\": 1, \"medium\": 2, \"hard\": 3}\n",
    "df[\"weight\"] = df[\"difficulty\"].map(difficulty_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ac0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_scores = []\n",
    "code_scores = []\n",
    "total_scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    gt_expl = row[\"solution_statement\"]\n",
    "    pred_expl = row[\"model_explanation\"]\n",
    "    gt_code = row[\"safe_code\"]\n",
    "    pred_code = row[\"model_fix\"]\n",
    "\n",
    "    if pred_expl.lower() == \"error\":\n",
    "        expl_score = 0\n",
    "    else:\n",
    "        emb_gt = encoder.encode(gt_expl, convert_to_tensor=True)\n",
    "        emb_pred = encoder.encode(pred_expl, convert_to_tensor=True)\n",
    "        sim = cosine_similarity([emb_gt.cpu().numpy()], [emb_pred.cpu().numpy()])[0][0]\n",
    "\n",
    "        if sim >= 0.9: expl_score = 1.0\n",
    "        elif sim >= 0.75: expl_score = 0.85\n",
    "        elif sim >= 0.6: expl_score = 0.6\n",
    "        elif sim >= 0.4: expl_score = 0.4\n",
    "        else: expl_score = 0.2\n",
    "\n",
    "    if not pred_code.strip() or pred_expl.lower() == \"error\":\n",
    "        code_score = 0\n",
    "    else:\n",
    "        sim = difflib.SequenceMatcher(None, gt_code, pred_code).ratio()\n",
    "        if sim >= 0.95: code_score = 1.0\n",
    "        elif sim >= 0.8: code_score = 0.85\n",
    "        elif sim >= 0.6: code_score = 0.6\n",
    "        elif sim >= 0.4: code_score = 0.4\n",
    "        else: code_score = 0.2\n",
    "\n",
    "    avg = (expl_score + code_score) / 2\n",
    "    final = avg * row[\"weight\"] * 100 / 3\n",
    "\n",
    "    explanation_scores.append(expl_score)\n",
    "    code_scores.append(code_score)\n",
    "    total_scores.append(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41b2862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d8f5238d784845ac7eb73335f3559d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec1c4b301a746c689cde44cb9224ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as bytes or binary IO objects is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset uploaded to https://huggingface.co/datasets/ZennyKenny/o4_mini-cosa-benchmark-results\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "model_name = \"o4_mini\"\n",
    "repo_id = f\"ZennyKenny/{model_name}-cosa-benchmark-results\"\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "assert hf_token, \"‚ùå HF_API_KEY is not set in your environment\"\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds.push_to_hub(repo_id, token=hf_token)\n",
    "\n",
    "print(f\"‚úÖ Dataset uploaded to https://huggingface.co/datasets/{repo_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91127713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå CoSA Benchmark Results for: O4_MINI\n",
      "==========================================\n",
      "üß† Explanation Quality:       61.12/100\n",
      "üîß Code Repair Quality:       85.55/100\n",
      "üèÅ Difficulty-Adjusted Score: 72.47/100\n"
     ]
    }
   ],
   "source": [
    "weights = {\"easy\": 1, \"medium\": 2, \"hard\": 3}\n",
    "\n",
    "if \"weight\" not in df.columns:\n",
    "    df[\"weight\"] = df[\"difficulty\"].map(weights)\n",
    "\n",
    "df[\"explanation_score_norm\"] = df[\"explanation_score\"]\n",
    "df[\"code_score_norm\"] = df[\"code_score\"]\n",
    "df[\"row_score\"] = (df[\"explanation_score_norm\"] + df[\"code_score_norm\"]) / 2\n",
    "\n",
    "df[\"weighted_score\"] = df[\"row_score\"] * df[\"weight\"]\n",
    "\n",
    "weighted_sum = df[\"weighted_score\"].sum()\n",
    "weight_total = df[\"weight\"].sum()\n",
    "difficulty_adjusted_score = (weighted_sum / weight_total) * 100\n",
    "\n",
    "avg_expl = df[\"explanation_score\"].mean() * 100\n",
    "avg_code = df[\"code_score\"].mean() * 100\n",
    "\n",
    "# üî• Summary print\n",
    "print(f\"üìå CoSA Benchmark Results for: {model_name.upper()}\")\n",
    "print(\"=\" * 42)\n",
    "print(f\"üß† Explanation Quality:       {avg_expl:.2f}/100\")\n",
    "print(f\"üîß Code Repair Quality:       {avg_code:.2f}/100\")\n",
    "print(f\"üèÅ Difficulty-Adjusted Score: {difficulty_adjusted_score:.2f}/100\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
